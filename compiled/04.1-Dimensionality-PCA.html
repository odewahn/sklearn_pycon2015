<p><small><i>This notebook was put together by <a href='http://www.vanderplas.com'>Jake Vanderplas</a> for PyCon 2015. Source and license info is on <a href='https://github.com/jakevdp/sklearn_pycon2015/'>GitHub</a>.</i></small></p>
<section data-type='chapter'>
<h1>Dimensionality Reduction: Principal Component Analysis in-depth</h1>

<p>Here we'll explore <strong>Principal Component Analysis</strong>, which is an extremely useful linear dimensionality reduction technique.</p>
<p>We'll start with our standard set of initial imports:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
from __future__ import print_function, division

%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# use seaborn plotting style defaults
import seaborn as sns; sns.set()
</pre>

<section data-type='sect1'>
<h1>Introducing Principal Component Analysis</h1>

<p>Principal Component Analysis is a very powerful unsupervised method for <em>dimensionality reduction</em> in data.  It's easiest to visualize by looking at a two-dimensional dataset:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
np.random.seed(1)
X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T
plt.plot(X[:, 0], X[:, 1], 'o')
plt.axis('equal');
</pre>

<p>We can see that there is a definite trend in the data. What PCA seeks to do is to find the <strong>Principal Axes</strong> in the data, and explain how important those axes are in describing the data distribution:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(X)
print(pca.explained_variance_)
print(pca.components_)
</pre>

<p>To see what these numbers mean, let's view them as vectors plotted on top of the data:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)
for length, vector in zip(pca.explained_variance_, pca.components_):
    v = vector * 3 * np.sqrt(length)
    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)
plt.axis('equal');
</pre>

<p>Notice that one vector is longer than the other. In a sense, this tells us that that direction in the data is somehow more "important" than the other direction.
The explained variance quantifies this measure of "importance" in direction.</p>
<p>Another way to think of it is that the second principal component could be <strong>completely ignored</strong> without much loss of information! Let's see what our data look like if we only keep 95% of the variance:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
clf = PCA(0.95) # keep 95% of variance
X_trans = clf.fit_transform(X)
print(X.shape)
print(X_trans.shape)
</pre>

<p>By specifying that we want to throw away 5% of the variance, the data is now compressed by a factor of 50%! Let's see what the data look like after this compression:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
X_new = clf.inverse_transform(X_trans)
plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.2)
plt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8)
plt.axis('equal');
</pre>

<p>The light points are the original data, while the dark points are the projected version.  We see that after truncating 5% of the variance of this dataset and then reprojecting it, the "most important" features of the data are maintained, and we've compressed the data by 50%!</p>
<p>This is the sense in which "dimensionality reduction" works: if you can approximate a data set in a lower dimension, you can often have an easier time visualizing it or fitting complicated models to the data.</p>
<section data-type='sect2'>
<h2>Application of PCA to Digits</h2>

<p>The dimensionality reduction might seem a bit abstract in two dimensions, but the projection and dimensionality reduction can be extremely useful when visualizing high-dimensional data.  Let's take a quick look at the application of PCA to the digits data we looked at before:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data
y = digits.target
</pre>

<pre data-code-language='python' data-executable='true' data-type='programlisting'>
pca = PCA(2)  # project from 64 to 2 dimensions
Xproj = pca.fit_transform(X)
print(X.shape)
print(Xproj.shape)
</pre>

<pre data-code-language='python' data-executable='true' data-type='programlisting'>
plt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', alpha=0.5,
            cmap=plt.cm.get_cmap('nipy_spectral', 10))
plt.colorbar();
</pre>

<p>This gives us an idea of the relationship between the digits. Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits, <strong>without reference</strong> to the labels.</p>
</section>
<section data-type='sect2'>
<h2>What do the Components Mean?</h2>

<p>PCA is a very useful dimensionality reduction algorithm, because it has a very intuitive interpretation via <em>eigenvectors</em>.
The input data is represented as a vector: in the case of the digits, our data is</p>
<p><span class='math-tex' data-type='tex'>\(
x = [x_1, x_2, x_3 \cdots]
\)</span></p>
<p>but what this really means is</p>
<p><span class='math-tex' data-type='tex'>\(
image(x) = x_1 \cdot{\rm (pixel~1)} + x_2 \cdot{\rm (pixel~2)} + x_3 \cdot{\rm (pixel~3)} \cdots
\)</span></p>
<p>If we reduce the dimensionality in the pixel space to (say) 6, we recover only a partial image:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
from fig_code.figures import plot_image_components

sns.set_style('white')
plot_image_components(digits.data[0])
</pre>

<p>But the pixel-wise representation is not the only choice. We can also use other <em>basis functions</em>, and write something like</p>
<p><span class='math-tex' data-type='tex'>\(
image(x) = {\rm mean} + x_1 \cdot{\rm (basis~1)} + x_2 \cdot{\rm (basis~2)} + x_3 \cdot{\rm (basis~3)} \cdots
\)</span></p>
<p>What PCA does is to choose optimal <strong>basis functions</strong> so that only a few are needed to get a reasonable approximation.
The low-dimensional representation of our data is the coefficients of this series, and the approximate reconstruction is the result of the sum:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
from fig_code.figures import plot_pca_interactive
plot_pca_interactive(digits.data)
</pre>

<p>Here we see that with only six PCA components, we recover a reasonable approximation of the input!</p>
<p>Thus we see that PCA can be viewed from two angles. It can be viewed as <strong>dimensionality reduction</strong>, or it can be viewed as a form of <strong>lossy data compression</strong> where the loss favors noise. In this way, PCA can be used as a <strong>filtering</strong> process as well.</p>
</section>
<section data-type='sect2'>
<h2>Choosing the Number of Components</h2>

<p>But how much information have we thrown away?  We can figure this out by looking at the <strong>explained variance</strong> as a function of the components:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
sns.set()
pca = PCA().fit(X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');
</pre>

<p>Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we'd need about 20 components to retain 90% of the variance.  Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations.</p>
</section>
<section data-type='sect2'>
<h2>PCA as data compression</h2>

<p>As we mentioned, PCA can be used for is a sort of data compression. Using a small <code>n_components</code> allows you to represent a high dimensional point as a sum of just a few principal vectors.</p>
<p>Here's what a single digit looks like as you change the number of components:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
fig, axes = plt.subplots(8, 8, figsize=(8, 8))
fig.subplots_adjust(hspace=0.1, wspace=0.1)

for i, ax in enumerate(axes.flat):
    pca = PCA(i + 1).fit(X)
    im = pca.inverse_transform(pca.transform(X[20:21]))

    ax.imshow(im.reshape((8, 8)), cmap='binary')
    ax.text(0.95, 0.05, 'n = {0}'.format(i + 1), ha='right',
            transform=ax.transAxes, color='green')
    ax.set_xticks([])
    ax.set_yticks([])
</pre>

<p>Let's take another look at this by using IPython's <code>interact</code> functionality to view the reconstruction of several images at once:</p>
<pre data-code-language='python' data-executable='true' data-type='programlisting'>
from IPython.html.widgets import interact

def plot_digits(n_components):
    fig = plt.figure(figsize=(8, 8))
    plt.subplot(1, 1, 1, frameon=False, xticks=[], yticks=[])
    nside = 10

    pca = PCA(n_components).fit(X)
    Xproj = pca.inverse_transform(pca.transform(X[:nside ** 2]))
    Xproj = np.reshape(Xproj, (nside, nside, 8, 8))
    total_var = pca.explained_variance_ratio_.sum()

    im = np.vstack([np.hstack([Xproj[i, j] for j in range(nside)])
                    for i in range(nside)])
    plt.imshow(im)
    plt.grid(False)
    plt.title("n = {0}, variance = {1:.2f}".format(n_components, total_var),
                 size=18)
    plt.clim(0, 16)

interact(plot_digits, n_components=[1, 64], nside=[1, 8]);
</pre>

</section>
</section>
<section data-type='sect1'>
<h1>Other Dimensionality Reducting Routines</h1>

<p>Note that scikit-learn contains many other unsupervised dimensionality reduction routines: some you might wish to try are
Other dimensionality reduction techniques which are useful to know about:</p>
<ul>
<li><a href='http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.PCA.html'>sklearn.decomposition.PCA</a>:
 Principal Component Analysis</li>
<li><a href='http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.RandomizedPCA.html'>sklearn.decomposition.RandomizedPCA</a>:
 extremely fast approximate PCA implementation based on a randomized algorithm</li>
<li><a href='http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.SparsePCA.html'>sklearn.decomposition.SparsePCA</a>:
 PCA variant including L1 penalty for sparsity</li>
<li><a href='http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.FastICA.html'>sklearn.decomposition.FastICA</a>:
 Independent Component Analysis</li>
<li><a href='http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.NMF.html'>sklearn.decomposition.NMF</a>:
 non-negative matrix factorization</li>
<li><a href='http://scikit-learn.org/0.13/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html'>sklearn.manifold.LocallyLinearEmbedding</a>:
 nonlinear manifold learning technique based on local neighborhood geometry</li>
<li><a href='http://scikit-learn.org/0.13/modules/generated/sklearn.manifold.Isomap.html'>sklearn.manifold.IsoMap</a>:
 nonlinear manifold learning technique based on a sparse graph algorithm</li>
</ul>
<p>Each of these has its own strengths &amp; weaknesses, and areas of application. You can read about them on the <a href='http://sklearn.org'>scikit-learn website</a>.</p>
</section>
</section>